{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830b7edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature mapping loaded\n",
      "Total features: 27\n"
     ]
    }
   ],
   "source": [
    "# Precise feature mapping\n",
    "FEATURE_MAPPING = {\n",
    "    # Personal info\n",
    "    'name': {'sheet': 'person_202510150920', 'column': 'name'},   \n",
    "    'sex': {'sheet': 'person_202510150920', 'column': 'sex'},\n",
    "    'nation': {'sheet': 'person_202510150920', 'column': 'nation'},\n",
    "    'region': {'sheet': 'person_202510150920', 'column': 'region'},\n",
    "    'priority': {'sheet': 'person_202510150920', 'column': 'priority'},\n",
    "    \n",
    "    # Student program info\n",
    "    'student_code': {'sheet': 'student_202510150856', 'column': 'student_code'},\n",
    "    'program_code': {'sheet': 'student_202510150856', 'column': 'program_code'},\n",
    "    'program_name': {'sheet': 'student_202510150856', 'column': 'program_name'},\n",
    "    'major_code': {'sheet': 'student_202510150856', 'column': 'major_code'},\n",
    "    'major_name': {'sheet': 'student_202510150856', 'column': 'major_name'},\n",
    "    'class_code': {'sheet': 'student_202510150856', 'column': 'class_code'},\n",
    "    'class_name': {'sheet': 'student_202510150856', 'column': 'class_name'},\n",
    "    \n",
    "    # Admission info\n",
    "    'admission_year': {'sheet': 'candidate_202510150926', 'column': 'admission_year'},\n",
    "    'admission_order': {'sheet': 'candidate_202510150926', 'column': 'admission_order'},\n",
    "    'admission_code': {'sheet': 'candidate_202510150926', 'column': 'admission_code'},\n",
    "    'admission_combination_code': {'sheet': 'candidate_202510150926', 'column': 'admission_combination_code'},\n",
    "    'admission_score': {'sheet': 'candidate_202510150926', 'column': 'admission_score'},\n",
    "    'encourage_point': {'sheet': 'candidate_202510150926', 'column': 'encourage_point'},\n",
    "    'subject_point_1': {'sheet': 'candidate_202510150926', 'column': 'subject_point_1'},\n",
    "    'subject_point_2': {'sheet': 'candidate_202510150926', 'column': 'subject_point_2'},\n",
    "    'subject_point_3': {'sheet': 'candidate_202510150926', 'column': 'subject_point_3'},\n",
    "    \n",
    "    # Person details\n",
    "    'avg_12': {'sheet': 'person_detail_202510150920', 'column': 'avg_12'},\n",
    "    'tt_province': {'sheet': 'person_detail_202510150920', 'column': 'tt_province'},\n",
    "    'tt_district': {'sheet': 'person_detail_202510150920', 'column': 'tt_district'},\n",
    "    'province_12_code': {'sheet': 'person_detail_202510150920', 'column': 'province_12_code'},\n",
    "    'school_12_code': {'sheet': 'person_detail_202510150920', 'column': 'school_12_code'},\n",
    "    \n",
    "    # Primary key\n",
    "    'person_code_hashed': {'sheet': 'student_202510150856', 'column': 'person_code_hashed'},\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Feature mapping loaded\")\n",
    "print(f\"Total features: {len(FEATURE_MAPPING)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825c70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ MEMORY-OPTIMIZED CHUNK PROCESSING (FIXED)\n",
      "üíæ Max RAM: ~500MB | Time: 3-7 min\n",
      "üìä LOADING REFERENCE KEYS...\n",
      "‚úÖ 18,876 unique person codes\n",
      "\n",
      "üîë BUILDING LOOKUP DICTIONARIES...\n",
      "   Processing sheet: person_202510150920\n",
      "     Total rows: 37,752\n",
      "‚úÖ person_202510150920: 18,864 records\n",
      "   Processing sheet: student_202510150856\n",
      "     Total rows: 37,752\n",
      "‚úÖ student_202510150856: 18,876 records\n",
      "   Processing sheet: candidate_202510150926\n",
      "     Total rows: 37,752\n",
      "‚úÖ candidate_202510150926: 16,580 records\n",
      "   Processing sheet: person_detail_202510150920\n",
      "     Total rows: 37,752\n",
      "‚úÖ person_detail_202510150920: 16,656 records\n",
      "\n",
      "üîó MERGING RECORDS...\n",
      "   Merging 0/18,876 (0.0%)\n",
      "‚úÖ Complete records: 16,580/18,876 (87.8%)\n",
      "\n",
      "üíæ SAVING FILES...\n",
      "‚úÖ CSV saved: 16,580 rows\n",
      "‚úÖ XLSX saved\n",
      "\n",
      "============================================================\n",
      "üéâ SUCCESS! CHUNK PROCESSING COMPLETE!\n",
      "üìä Final: 16,580 students √ó 27 features\n",
      "üìÅ Files: data_prepared/student_infor.csv\n",
      "üìÅ Files: data_prepared/student_infor.xlsx\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Input and output paths\n",
    "file_path = 'base_data/student_hashed.xlsx'\n",
    "output_xlsx = 'data_prepared/student_infor.xlsx'\n",
    "output_csv = 'data_prepared/student_infor.csv'\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('data_prepared', exist_ok=True)\n",
    "\n",
    "print(\"üîÑ MEMORY-OPTIMIZED CHUNK PROCESSING (FIXED)\")\n",
    "print(\"üíæ Max RAM: ~500MB | Time: 3-7 min\")\n",
    "\n",
    "# Feature groups by sheet\n",
    "feature_groups = {\n",
    "    'person_202510150920': ['name', 'sex', 'nation', 'region', 'priority'],\n",
    "    'student_202510150856': ['student_code', 'program_code', 'program_name', 'major_code', 'major_name', 'class_code', 'class_name'],\n",
    "    'candidate_202510150926': ['admission_year', 'admission_order', 'admission_code', 'admission_combination_code', 'admission_score', 'encourage_point', 'subject_point_1', 'subject_point_2', 'subject_point_3'],\n",
    "    'person_detail_202510150920': ['avg_12', 'tt_province', 'tt_district', 'province_12_code', 'school_12_code'],\n",
    "}\n",
    "\n",
    "CHUNK_SIZE = 50000\n",
    "EXCEL_ROW_LIMIT = 1048576\n",
    "\n",
    "print(\"üìä LOADING REFERENCE KEYS...\")\n",
    "xl = pd.ExcelFile(file_path)\n",
    "student_df = pd.read_excel(xl, 'student_202510150856', usecols=['person_code_hashed'])\n",
    "student_df['person_code_hashed'] = student_df['person_code_hashed'].astype(str).str.strip()\n",
    "keys = student_df[student_df['person_code_hashed'].notna() & (student_df['person_code_hashed'] != '')]['person_code_hashed'].unique()\n",
    "print(f\"‚úÖ {len(keys):,} unique person codes\")\n",
    "\n",
    "# Step 2: Build lookup dictionaries with MANUAL CHUNKING\n",
    "print(\"\\nüîë BUILDING LOOKUP DICTIONARIES...\")\n",
    "lookups = {}\n",
    "\n",
    "def read_sheet_chunks(xl, sheet, cols, keys_set, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"Manual chunking using skiprows + nrows\"\"\"\n",
    "    lookup = {}\n",
    "    total_rows = 0\n",
    "    \n",
    "    # Get total rows in sheet (first read with nrows=1 to get shape)\n",
    "    try:\n",
    "        test_df = pd.read_excel(xl, sheet_name=sheet, nrows=1, usecols=cols)\n",
    "        # Estimate total rows by reading until empty\n",
    "        row_count = 0\n",
    "        while True:\n",
    "            chunk = pd.read_excel(xl, sheet_name=sheet, skiprows=row_count, nrows=chunk_size, usecols=cols)\n",
    "            if chunk.empty:\n",
    "                break\n",
    "            row_count += len(chunk)\n",
    "        total_rows = row_count\n",
    "    except:\n",
    "        total_rows = len(keys) * 2  # Fallback\n",
    "    \n",
    "    print(f\"     Total rows: {total_rows:,}\")\n",
    "    \n",
    "    # Process chunks\n",
    "    for start_row in range(0, total_rows, chunk_size):\n",
    "        chunk = pd.read_excel(xl, sheet_name=sheet, skiprows=start_row, nrows=chunk_size, usecols=cols)\n",
    "        \n",
    "        if chunk.empty:\n",
    "            break\n",
    "            \n",
    "        # Clean key\n",
    "        chunk['person_code_hashed'] = chunk['person_code_hashed'].astype(str).str.strip()\n",
    "        chunk = chunk[chunk['person_code_hashed'].isin(keys_set)]\n",
    "        \n",
    "        # Build lookup\n",
    "        for _, row in chunk.iterrows():\n",
    "            key = row['person_code_hashed']\n",
    "            if key not in lookup:\n",
    "                lookup[key] = {}\n",
    "            for col in cols:\n",
    "                if col != 'person_code_hashed':\n",
    "                    lookup[key][col] = row[col]\n",
    "        \n",
    "        if (start_row // chunk_size) % 10 == 0:\n",
    "            print(f\"     Processed {start_row:,}/{total_rows:,} rows...\", end='\\r')\n",
    "    \n",
    "    return lookup\n",
    "\n",
    "# Add key column to each group\n",
    "for sheet, cols in feature_groups.items():\n",
    "    cols.append('person_code_hashed')\n",
    "\n",
    "keys_set = set(keys)  # Faster lookup\n",
    "\n",
    "for sheet, cols in feature_groups.items():\n",
    "    print(f\"   Processing sheet: {sheet}\")\n",
    "    lookups[sheet] = read_sheet_chunks(xl, sheet, cols, keys_set)\n",
    "    print(f\"‚úÖ {sheet}: {len(lookups[sheet]):,} records\")\n",
    "\n",
    "# Step 3: MERGE using dictionaries\n",
    "print(\"\\nüîó MERGING RECORDS...\")\n",
    "final_data = []\n",
    "feature_order = [\n",
    "    'name', 'sex', 'nation', 'region', 'priority',\n",
    "    'student_code', 'program_code', 'program_name', 'major_code', 'major_name', 'class_code', 'class_name',\n",
    "    'admission_year', 'admission_order', 'admission_code', 'admission_combination_code', 'admission_score', \n",
    "    'encourage_point', 'subject_point_1', 'subject_point_2', 'subject_point_3',\n",
    "    'avg_12', 'tt_province', 'tt_district', 'province_12_code', 'school_12_code',\n",
    "    'person_code_hashed'\n",
    "]\n",
    "\n",
    "total_complete = 0\n",
    "for i, key in enumerate(keys):\n",
    "    if i % 100000 == 0:\n",
    "        print(f\"   Merging {i:,}/{len(keys):,} ({i/len(keys)*100:.1f}%)\", end='\\r')\n",
    "    \n",
    "    # Check if ALL sheets have this key\n",
    "    record_complete = True\n",
    "    merged_row = {'person_code_hashed': key}\n",
    "    \n",
    "    for sheet in feature_groups.keys():\n",
    "        if key in lookups[sheet]:\n",
    "            merged_row.update(lookups[sheet][key])\n",
    "        else:\n",
    "            record_complete = False\n",
    "            break\n",
    "    \n",
    "    if record_complete:\n",
    "        ordered_row = {k: merged_row.get(k, None) for k in feature_order}\n",
    "        final_data.append(ordered_row)\n",
    "        total_complete += 1\n",
    "\n",
    "print(f\"\\n‚úÖ Complete records: {total_complete:,}/{len(keys):,} ({total_complete/len(keys)*100:.1f}%)\")\n",
    "\n",
    "# Step 4: Save\n",
    "print(\"\\nüíæ SAVING FILES...\")\n",
    "df_final = pd.DataFrame(final_data)\n",
    "\n",
    "# Save CSV\n",
    "df_final.to_csv(output_csv, index=False)\n",
    "print(f\"‚úÖ CSV saved: {len(df_final):,} rows\")\n",
    "\n",
    "# Save XLSX if fits\n",
    "if len(df_final) <= EXCEL_ROW_LIMIT:\n",
    "    df_final.to_excel(output_xlsx, index=False, engine='openpyxl')\n",
    "    print(f\"‚úÖ XLSX saved\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  XLSX skipped: {len(df_final):,} > {EXCEL_ROW_LIMIT:,} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ SUCCESS! CHUNK PROCESSING COMPLETE!\")\n",
    "print(f\"üìä Final: {len(df_final):,} students √ó {len(feature_order)} features\")\n",
    "print(f\"üìÅ Files: {output_csv}\")\n",
    "if os.path.exists(output_xlsx):\n",
    "    print(f\"üìÅ Files: {output_xlsx}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2af75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
