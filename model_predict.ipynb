{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ecaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import optuna\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae2b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =========================================\n",
    "# Model LSTM đã được sửa lỗi và tối ưu\n",
    "# =========================================\n",
    "class AdvancedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_sizes=[64, 32], dropout=0.2, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_layers = len(hidden_sizes)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Stacked LSTM layers\n",
    "        self.lstm_layers = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            input_sz = input_size if i == 0 else hidden_sizes[i-1]\n",
    "            self.lstm_layers.append(nn.LSTM(input_sz, hidden_sizes[i], \n",
    "                                          batch_first=True, dropout=dropout if i < self.num_layers-1 else 0))\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn = nn.BatchNorm1d(hidden_sizes[-1])\n",
    "        \n",
    "        # Output layers với dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_sizes[-1], 32)\n",
    "        self.fc2 = nn.Linear(32, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden states nếu cần\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # Qua các LSTM layers - SỬA LỖI: chỉ lấy output cuối của layer cuối\n",
    "        lstm_out = x\n",
    "        for i, lstm in enumerate(self.lstm_layers):\n",
    "            lstm_out, hidden[i] = lstm(lstm_out, hidden[i])\n",
    "            if i == self.num_layers - 1:  # Chỉ layer cuối mới lấy output cuối\n",
    "                x = lstm_out[:, -1, :]  # (batch, hidden_size)\n",
    "            else:\n",
    "                x = lstm_out  # Giữ nguyên sequence cho layer tiếp theo\n",
    "        \n",
    "        # Batch norm và fully connected\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(self.fc1(self.dropout(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = []\n",
    "        for size in self.hidden_sizes:\n",
    "            h0 = torch.zeros(1, batch_size, size, device=self.device)\n",
    "            c0 = torch.zeros(1, batch_size, size, device=self.device)\n",
    "            hidden.append((h0, c0))\n",
    "        return hidden\n",
    "\n",
    "# =========================================\n",
    "# Data preprocessing với multivariate support\n",
    "# =========================================\n",
    "def load_and_split_data(file_path='data.csv', seq_length=3, test_size=0.1, val_size=0.1):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Xử lý multivariate data\n",
    "    if 'Registrations' in df.columns:\n",
    "        target_col = 'Registrations'\n",
    "        # Tìm tất cả cột điểm (Diem_)\n",
    "        feature_cols = [target_col]\n",
    "        diem_cols = [col for col in df.columns if col.startswith('Diem_')]\n",
    "        feature_cols.extend(diem_cols)\n",
    "        \n",
    "        print(f\"Using features: {feature_cols}\")\n",
    "        \n",
    "        # Điền missing values\n",
    "        df[feature_cols] = df[feature_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        features_scaled = scaler.fit_transform(df[feature_cols].values)\n",
    "        \n",
    "        # Target index\n",
    "        target_idx = feature_cols.index(target_col)\n",
    "        \n",
    "    else:\n",
    "        # Fallback to original single feature\n",
    "        print(\"Using single feature 'Registrations'\")\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        features_scaled = scaler.fit_transform(df['Registrations'].values.reshape(-1, 1))\n",
    "        target_idx = 0\n",
    "    \n",
    "    years = df['Year'].values\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(features_scaled) - seq_length):\n",
    "        X.append(features_scaled[i:i+seq_length])\n",
    "        y.append(features_scaled[i+seq_length, target_idx])  # Predict target\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Temporal split (không shuffle)\n",
    "    n_samples = len(X)\n",
    "    test_start = int(n_samples * (1 - test_size))\n",
    "    val_start = int(test_start * (1 - val_size))\n",
    "    \n",
    "    X_train = X[:val_start]\n",
    "    X_val = X[val_start:test_start]\n",
    "    X_test = X[test_start:]\n",
    "    \n",
    "    y_train = y[:val_start]\n",
    "    y_val = y[val_start:test_start]\n",
    "    y_test = y[test_start:]\n",
    "    \n",
    "    print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    print(f\"Input features: {X.shape[2]}\")\n",
    "    \n",
    "    return (torch.FloatTensor(X_train), torch.FloatTensor(y_train),\n",
    "            torch.FloatTensor(X_val), torch.FloatTensor(y_val),\n",
    "            torch.FloatTensor(X_test), torch.FloatTensor(y_test),\n",
    "            scaler, years, features_scaled, target_idx)\n",
    "\n",
    "# =========================================\n",
    "# EarlyStopping đã hoàn thiện\n",
    "# =========================================\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=1e-4, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        self.best_epoch = 0\n",
    "    \n",
    "    def __call__(self, val_loss, model, epoch):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "            self.best_epoch = epoch\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "            self.best_epoch = epoch\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "                print(f\"Restored best weights from epoch {self.best_epoch}\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "# =========================================\n",
    "# Training function hoàn chỉnh\n",
    "# =========================================\n",
    "def train_model(X_train, y_train, X_val, y_val, input_size, epochs=300, batch_size=16, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Training on {device}\")\n",
    "    \n",
    "    model = AdvancedLSTMModel(input_size=input_size, hidden_sizes=[64, 32], dropout=0.2).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=20, min_delta=1e-4)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            batch_X = X_train[i:i+batch_size].to(device)\n",
    "            batch_y = y_train[i:i+batch_size].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_train_loss = train_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs, _ = model(X_val.to(device))\n",
    "            val_loss = criterion(val_outputs, y_val.to(device)).item()\n",
    "            val_losses.append(val_loss)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Logging\n",
    "        if epoch % 20 == 0 or epoch < 10:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'Epoch {epoch:3d}: Train={avg_train_loss:.6f}, Val={val_loss:.6f}, LR={current_lr:.2e}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping(val_loss, model, epoch):\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    # Save final best model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': early_stopping.best_loss,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses\n",
    "    }, 'advanced_lstm_model.pth')\n",
    "    \n",
    "    print(f\"Best validation loss: {early_stopping.best_loss:.6f}\")\n",
    "    print(\"Model checkpoint saved to 'advanced_lstm_model.pth'\")\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# =========================================\n",
    "# Evaluation function cải tiến\n",
    "# =========================================\n",
    "def evaluate_model(model, X_test, y_test, scaler, feature_cols, target_idx):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_pred = model(X_test.to(device))[0].cpu().numpy()\n",
    "        y_test_np = y_test.numpy()\n",
    "        \n",
    "        # Inverse transform cho multivariate\n",
    "        if len(feature_cols) > 1:\n",
    "            # Tạo dummy features cho inverse transform\n",
    "            dummy_features = np.zeros((len(test_pred), len(feature_cols)))\n",
    "            dummy_features[:, target_idx] = test_pred.flatten()\n",
    "            test_pred_orig = scaler.inverse_transform(dummy_features)[:, target_idx]\n",
    "            \n",
    "            dummy_true = np.zeros((len(y_test_np), len(feature_cols)))\n",
    "            dummy_true[:, target_idx] = y_test_np\n",
    "            y_test_orig = scaler.inverse_transform(dummy_true)[:, target_idx]\n",
    "        else:\n",
    "            test_pred_orig = scaler.inverse_transform(test_pred)\n",
    "            y_test_orig = scaler.inverse_transform(y_test_np.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Metrics\n",
    "        mse = mean_squared_error(y_test_orig, test_pred_orig)\n",
    "        mae = mean_absolute_error(y_test_orig, test_pred_orig)\n",
    "        mape = np.mean(np.abs((y_test_orig - test_pred_orig) / (y_test_orig + 1e-8))) * 100\n",
    "        \n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        print(f\"Test Results:\")\n",
    "        print(f\"  MSE: {mse:,.2f}\")\n",
    "        print(f\"  RMSE: {rmse:,.2f}\")\n",
    "        print(f\"  MAE: {mae:,.2f}\")\n",
    "        print(f\"  MAPE: {mape:.2f}%\")\n",
    "        \n",
    "        # R² score\n",
    "        ss_res = np.sum((y_test_orig - test_pred_orig) ** 2)\n",
    "        ss_tot = np.sum((y_test_orig - np.mean(y_test_orig)) ** 2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        print(f\"  R²: {r2:.4f}\")\n",
    "    \n",
    "    return test_pred_orig, y_test_orig\n",
    "\n",
    "# =========================================\n",
    "# Multi-step prediction cải tiến\n",
    "# =========================================\n",
    "def predict_future(model, scaler, last_sequence, future_years=[2025, 2026, 2027], \n",
    "                  feature_cols=None, target_idx=0, num_features=1):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    predictions = []\n",
    "    current_seq = last_sequence.copy().astype(np.float32)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, year in enumerate(future_years):\n",
    "            input_seq = torch.FloatTensor(current_seq).unsqueeze(0).to(device)\n",
    "            pred, _ = model(input_seq)\n",
    "            pred_val = pred.cpu().numpy()[0, 0]\n",
    "            predictions.append(pred_val)\n",
    "            \n",
    "            # Update sequence cho multi-step\n",
    "            if num_features > 1:\n",
    "                # Pad dummy values cho other features (có thể cải tiến bằng trend extrapolation)\n",
    "                new_row = np.zeros(num_features)\n",
    "                new_row[target_idx] = pred_val\n",
    "                # Copy last values của other features\n",
    "                for j in range(num_features):\n",
    "                    if j != target_idx:\n",
    "                        new_row[j] = current_seq[-1, j]\n",
    "                current_seq = np.vstack([current_seq[1:], new_row])\n",
    "            else:\n",
    "                current_seq = np.append(current_seq[1:], pred_val).reshape(-1, 1)\n",
    "    \n",
    "    # Inverse scale\n",
    "    if num_features > 1:\n",
    "        dummy_features = np.zeros((len(predictions), num_features))\n",
    "        dummy_features[:, target_idx] = predictions\n",
    "        predictions_orig = scaler.inverse_transform(dummy_features)[:, target_idx]\n",
    "    else:\n",
    "        predictions_orig = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return predictions_orig\n",
    "\n",
    "# =========================================\n",
    "# Plotting function cải tiến\n",
    "# =========================================\n",
    "def plot_results(train_losses, val_losses, years, registrations, future_years, predictions, \n",
    "                test_pred=None, test_true=None, save_fig=False):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training curves\n",
    "    ax1.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "    ax1.plot(val_losses, label='Val Loss', alpha=0.7)\n",
    "    ax1.set_title('Training & Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Main prediction plot\n",
    "    ax2.plot(years, registrations, 'o-', label='Historical Data', linewidth=2, markersize=6)\n",
    "    if test_pred is not None and test_true is not None:\n",
    "        test_years = years[-len(test_pred):]\n",
    "        ax2.plot(test_years, test_true, 's', label='Test True', markersize=8, alpha=0.8)\n",
    "        ax2.plot(test_years, test_pred, 'x', label='Test Prediction', markersize=8, alpha=0.8)\n",
    "    ax2.plot(future_years, predictions, 'o--', label='Future Prediction', \n",
    "             linewidth=3, markersize=10, color='red')\n",
    "    ax2.set_title('Registration Trends Prediction')\n",
    "    ax2.set_xlabel('Year')\n",
    "    ax2.set_ylabel('Registrations')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals plot\n",
    "    if test_pred is not None and test_true is not None:\n",
    "        residuals = test_true - test_pred\n",
    "        ax3.scatter(test_true, residuals, alpha=0.6)\n",
    "        ax3.axhline(y=0, color='r', linestyle='--')\n",
    "        ax3.set_title('Residuals Plot')\n",
    "        ax3.set_xlabel('True Values')\n",
    "        ax3.set_ylabel('Residuals')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Prediction vs Actual\n",
    "    if test_pred is not None and test_true is not None:\n",
    "        ax4.scatter(test_true, test_pred, alpha=0.7)\n",
    "        min_val = min(test_true.min(), test_pred.min())\n",
    "        max_val = max(test_true.max(), test_pred.max())\n",
    "        ax4.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "        ax4.set_title('Predicted vs Actual')\n",
    "        ax4.set_xlabel('Actual')\n",
    "        ax4.set_ylabel('Predicted')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_fig:\n",
    "        plt.savefig('model_results.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"Results saved to 'model_results.png'\")\n",
    "    plt.show()\n",
    "\n",
    "# =========================================\n",
    "# Main execution hoàn chỉnh\n",
    "# =========================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Advanced LSTM Registration Prediction ===\")\n",
    "    \n",
    "    try:\n",
    "        # Load và split data\n",
    "        (X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "         scaler, years, features_scaled, target_idx) = load_and_split_data('data.csv', seq_length=3)\n",
    "        \n",
    "        input_size = features_scaled.shape[1]\n",
    "        num_features = input_size\n",
    "        \n",
    "        # Train model\n",
    "        print(\"\\n--- Training Model ---\")\n",
    "        model, train_losses, val_losses = train_model(\n",
    "            X_train, y_train, X_val, y_val, \n",
    "            input_size=input_size,\n",
    "            epochs=300,\n",
    "            batch_size=min(32, len(X_train))  # Adaptive batch size\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        print(\"\\n--- Evaluation ---\")\n",
    "        feature_cols = ['Registrations'] + [f'Diem_{i}' for i in range(10)]  # Adjust based on your data\n",
    "        test_pred_orig, test_true_orig = evaluate_model(\n",
    "            model, X_test, y_test, scaler, feature_cols, target_idx\n",
    "        )\n",
    "        \n",
    "        # Future predictions\n",
    "        print(\"\\n--- Future Predictions ---\")\n",
    "        seq_length = 3\n",
    "        last_seq = features_scaled[-seq_length:]\n",
    "        future_years = list(range(years[-1] + 1, years[-1] + 4))  # Next 3 years\n",
    "        future_preds = predict_future(\n",
    "            model, scaler, last_seq, future_years, \n",
    "            feature_cols, target_idx, num_features\n",
    "        )\n",
    "        \n",
    "        print(\"\\nFuture Predictions:\")\n",
    "        for year, pred in zip(future_years, future_preds):\n",
    "            print(f\"Year {year}: {int(pred):,} registrations\")\n",
    "        \n",
    "        # Historical registrations cho plot\n",
    "        historical_registrations = scaler.inverse_transform(\n",
    "            np.column_stack([features_scaled[:, target_idx], \n",
    "                           np.zeros((len(features_scaled), num_features-1))])\n",
    "        )[:, 0]\n",
    "        \n",
    "        # Plot results\n",
    "        print(\"\\n--- Plotting Results ---\")\n",
    "        plot_results(train_losses, val_losses, years, historical_registrations,\n",
    "                    future_years, future_preds, test_pred_orig, test_true_orig, save_fig=True)\n",
    "        \n",
    "        print(\"\\n=== Training completed successfully! ===\")\n",
    "        print(\"Model saved as 'advanced_lstm_model.pth'\")\n",
    "        print(\"To load later: torch.load('advanced_lstm_model.pth')\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'data.csv' not found!\")\n",
    "        print(\"Expected format:\")\n",
    "        print(\"Year,Registrations,Diem_Toan,Diem_Ly,Diem_Hoa,...\")\n",
    "        print(\"2015,800000,7.5,7.2,6.8,...\")\n",
    "        print(\"Create data.csv and try again.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {str(e)}\")\n",
    "        print(\"Check your data format and try again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
